{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Project 2 AI Chat Notebook - \n",
    "\n",
    "In this notebook, we want to see your interaction with ChatGPT that you use to solve\n",
    "the unsupervised AND supervised machine learning tasks in `project2.ipynb`.  \n",
    "\n",
    "\n",
    "# ChatGPT functions to Use\n",
    "\n",
    "You are free to use these functions, or other LLMs, or the Web ChatGPT, as long as you show those systems' outputs\n",
    "so we are convinced you have used a large language model to perform the machine learning tasks.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb127947ae786d3e"
  },
  {
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "\tbase_url = 'http://aitools.cs.vt.edu:7860/openai/v1',\n",
    "    api_key=\"aitools\"\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T03:53:52.681675Z",
     "start_time": "2024-09-09T03:53:52.647270Z"
    }
   },
   "id": "575b42125efef914",
   "outputs": [],
   "execution_count": 109
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def converse(prompt, messages=None, \n",
    "             model=\"gpt-3.5-turbo\", \n",
    "             max_tokens=4000, \n",
    "             temperature=0, \n",
    "             top_p=1, \n",
    "             frequency_penalty=0,\n",
    "\t\t\t presence_penalty=0):\n",
    "    # Add the user's message to the list of messages\n",
    "    if messages is None:\n",
    "        messages = []\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "                model=model,\n",
    "\t\tmessages=messages,\n",
    "\t\ttemperature=temperature,\n",
    "\t\tmax_tokens=max_tokens,\n",
    "\t\ttop_p=top_p,\n",
    "\t\tfrequency_penalty=frequency_penalty,\n",
    "\t\tpresence_penalty=presence_penalty).choices[0].message.content\n",
    "\n",
    "    # Add the assistant's message to the list of messages\n",
    "    messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "    return response, messages"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T03:53:54.509675Z",
     "start_time": "2024-09-09T03:53:54.504078Z"
    }
   },
   "id": "4b1b57a91e19d6f7",
   "outputs": [],
   "execution_count": 110
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Unsupervised Learning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33f340e736f29545"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this section, show us how you performed the unsupervised learning tasks with the help of Chat GPT. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b35358120e31f"
  },
  {
   "cell_type": "code",
   "source": [
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T03:53:58.154934Z",
     "start_time": "2024-09-09T03:53:58.144994Z"
    }
   },
   "id": "c81794323049a9e",
   "outputs": [],
   "execution_count": 111
  },
  {
   "cell_type": "code",
   "source": [
    "prompt = \"\"\"\n",
    "- Load all data files matching 'data/datasets/train-00000-5k-*.parquet'.\n",
    "- Merge those dataframes into a single dataframe \"df\".\n",
    "\"\"\"\n",
    "response, messages = converse(prompt, [])\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T03:54:00.945001Z",
     "start_time": "2024-09-09T03:53:59.590893Z"
    }
   },
   "id": "5c1d92ebffba7018",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "import glob\n",
      "\n",
      "# Load all data files matching the pattern\n",
      "files = glob.glob('data/datasets/train-00000-5k-*.parquet')\n",
      "\n",
      "# Merge all dataframes into a single dataframe\n",
      "df = pd.concat([pd.read_parquet(file) for file in files], ignore_index=True)\n",
      "\n",
      "# Display the merged dataframe\n",
      "print(df.head())\n"
     ]
    }
   ],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T03:54:05.505627Z",
     "start_time": "2024-09-09T03:54:03.916291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"\"\"\n",
    "- Define the code snippets (from data frames's \"code\" attribute)\n",
    "\"\"\"\n",
    "response, messages = converse(prompt, messages)\n",
    "print(response)"
   ],
   "id": "704691bc11529f73",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To define the code snippets from the data frames' \"code\" attribute, you can use the following code:\n",
      "\n",
      "```python\n",
      "# Define the code snippets from the data frames' \"code\" attribute\n",
      "code_snippets = df['code'].tolist()\n",
      "\n",
      "# Display the code snippets\n",
      "for snippet in code_snippets:\n",
      "    print(snippet)\n",
      "```\n",
      "\n",
      "This code snippet will extract the code snippets from the \"code\" attribute of the data frames in the merged dataframe \"df\" and display them one by one.\n"
     ]
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T03:54:09.954111Z",
     "start_time": "2024-09-09T03:54:08.590037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"\"\"\n",
    "- Define labels (from dataframe's \"language\" attribute).\n",
    "\"\"\"\n",
    "response, messages = converse(prompt, messages)\n",
    "print(response)"
   ],
   "id": "35ad09e43da04b4d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To define the labels from the dataframe's \"language\" attribute, you can use the following code:\n",
      "\n",
      "```python\n",
      "# Define the labels from the dataframe's \"language\" attribute\n",
      "labels = df['language'].tolist()\n",
      "\n",
      "# Display the labels\n",
      "print(labels)\n",
      "```\n",
      "\n",
      "This code snippet will extract the labels from the \"language\" attribute of the merged dataframe \"df\" and store them in a list called \"labels\". It will then display the list of labels.\n"
     ]
    }
   ],
   "execution_count": 114
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T03:54:17.259876Z",
     "start_time": "2024-09-09T03:54:13.093531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"\"\"\n",
    "- Convert code snippets to TF-IDF features\n",
    "\"\"\"\n",
    "response, messages = converse(prompt, messages)\n",
    "print(response)"
   ],
   "id": "d0d1abacc431d065",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To convert the code snippets to TF-IDF features, you can use the following code snippet:\n",
      "\n",
      "```python\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "# Initialize the TF-IDF vectorizer\n",
      "tfidf_vectorizer = TfidfVectorizer()\n",
      "\n",
      "# Fit and transform the code snippets to TF-IDF features\n",
      "tfidf_features = tfidf_vectorizer.fit_transform(code_snippets)\n",
      "\n",
      "# Display the shape of the TF-IDF features\n",
      "print(\"TF-IDF features shape:\", tfidf_features.shape)\n",
      "```\n",
      "\n",
      "This code snippet uses the `TfidfVectorizer` from scikit-learn to convert the code snippets stored in the `code_snippets` list to TF-IDF features. It fits the vectorizer on the code snippets and transforms them to TF-IDF features. Finally, it displays the shape of the TF-IDF features matrix.\n"
     ]
    }
   ],
   "execution_count": 115
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T03:54:23.476235Z",
     "start_time": "2024-09-09T03:54:21.640176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"\"\"\n",
    "- Perform K-Means clustering on the TF-IDF features. Use 5 clusters.\n",
    "\"\"\"\n",
    "response, messages = converse(prompt, messages)\n",
    "print(response)"
   ],
   "id": "30aaab5970e4b6b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To perform K-Means clustering on the TF-IDF features with 5 clusters, you can use the following code snippet:\n",
      "\n",
      "```python\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Perform K-Means clustering with 5 clusters\n",
      "kmeans = KMeans(n_clusters=5, random_state=42)\n",
      "cluster_labels = kmeans.fit_predict(tfidf_features)\n",
      "\n",
      "# Display the cluster labels\n",
      "print(\"Cluster labels:\", cluster_labels)\n",
      "```\n",
      "\n",
      "This code snippet uses the `KMeans` algorithm from scikit-learn to perform clustering on the TF-IDF features with 5 clusters. It assigns each code snippet to one of the 5 clusters and stores the cluster labels in the `cluster_labels` array. Finally, it displays the cluster labels assigned to each code snippet.\n"
     ]
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T03:54:31.729230Z",
     "start_time": "2024-09-09T03:54:28.888172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"\"\"\n",
    "- Show 10 sample code snippets truncated to 80 characters for each identified cluster. If code has newlines, replace them with spaces.\n",
    "\"\"\"\n",
    "response, messages = converse(prompt, messages)\n",
    "print(response)"
   ],
   "id": "58945beab9d558df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To show 10 sample code snippets truncated to 80 characters for each identified cluster (replacing newlines with spaces), you can use the following code snippet:\n",
      "\n",
      "```python\n",
      "import textwrap\n",
      "\n",
      "# Create a DataFrame with code snippets and cluster labels\n",
      "df_clustered = pd.DataFrame({'code': code_snippets, 'cluster': cluster_labels})\n",
      "\n",
      "# Truncate code snippets to 80 characters and replace newlines with spaces\n",
      "df_clustered['code_truncated'] = df_clustered['code'].str.replace('\\n', ' ').apply(lambda x: textwrap.shorten(x, width=80))\n",
      "\n",
      "# Display 10 sample code snippets for each identified cluster\n",
      "for cluster_id in range(5):\n",
      "    cluster_samples = df_clustered[df_clustered['cluster'] == cluster_id]['code_truncated'].head(10).tolist()\n",
      "    print(f\"Cluster {cluster_id} samples:\")\n",
      "    for sample in cluster_samples:\n",
      "        print(sample)\n",
      "    print()\n",
      "```\n",
      "\n",
      "This code snippet creates a DataFrame with code snippets and their corresponding cluster labels. It then truncates the code snippets to 80 characters and replaces newlines with spaces. Finally, it displays 10 sample code snippets for each identified cluster, showing them truncated to 80 characters.\n"
     ]
    }
   ],
   "execution_count": 117
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Showing Unsupervised Clustering Visually (Optional)",
   "id": "f4d2afd9c026c929"
  },
  {
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T02:02:17.259632Z",
     "start_time": "2024-09-09T02:02:14.777937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"\"\"\n",
    "- Generate code to make a plot (use mathplotlib) showing the data points and clusters. Use a distinct color for each cluster.\n",
    "\"\"\"\n",
    "response, messages = converse(prompt, messages)\n",
    "print(response)"
   ],
   "id": "f16a068044bb54d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Assuming the dataframe has a column 'cluster' indicating the cluster for each data point\n",
      "clusters = df['cluster'].unique()\n",
      "\n",
      "# Define colors for each cluster\n",
      "colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k']\n",
      "\n",
      "# Plot data points with different colors for each cluster\n",
      "plt.figure(figsize=(10, 6))\n",
      "for i, cluster in enumerate(clusters):\n",
      "    cluster_data = df[df['cluster'] == cluster]\n",
      "    plt.scatter(cluster_data['x'], cluster_data['y'], color=colors[i], label=f'Cluster {cluster}')\n",
      "\n",
      "plt.xlabel('X')\n",
      "plt.ylabel('Y')\n",
      "plt.title('Data Points and Clusters')\n",
      "plt.legend()\n",
      "plt.show()\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Supervised Learning\n",
    "\n",
    "In this section, show us how you performed the supervised learning tasks with the help of Chat GPT or other LLM."
   ],
   "id": "c70d1582f9d2527c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T03:54:58.067036Z",
     "start_time": "2024-09-09T03:54:58.058975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import glob\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n"
   ],
   "id": "a66bc4d6a364541c",
   "outputs": [],
   "execution_count": 118
  },
  {
   "cell_type": "code",
   "source": [
    "prompt = \"\"\"\n",
    "- Track measurements for each round: the round number/index, how long in seconds the train, predict steps take, and an accuracy report\n",
    "- Use a list of dictionaries for measurement storage, with \"round\", \"train\", \"predict\", \"accuracy\" as keys\n",
    "\"\"\"\n",
    "response, messages = converse(prompt, [])\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T03:55:12.905995Z",
     "start_time": "2024-09-09T03:55:10.214486Z"
    }
   },
   "id": "1190d1f0b12ab547",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "# Initialize list to store measurements\n",
      "measurements = []\n",
      "\n",
      "# Function to track measurements for each round\n",
      "def track_measurements(round_num, train_time, predict_steps, accuracy):\n",
      "    measurement = {\n",
      "        \"round\": round_num,\n",
      "        \"train\": train_time,\n",
      "        \"predict\": predict_steps,\n",
      "        \"accuracy\": accuracy\n",
      "    }\n",
      "    measurements.append(measurement)\n",
      "\n",
      "# Example usage\n",
      "track_measurements(1, 30, 1000, 0.85)\n",
      "track_measurements(2, 25, 900, 0.88)\n",
      "\n",
      "# Print measurements\n",
      "for measurement in measurements:\n",
      "    print(measurement)\n",
      "```\n",
      "\n",
      "Output:\n",
      "```\n",
      "{'round': 1, 'train': 30, 'predict': 1000, 'accuracy': 0.85}\n",
      "{'round': 2, 'train': 25, 'predict': 900, 'accuracy': 0.88}\n",
      "```\n"
     ]
    }
   ],
   "execution_count": 119
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T03:55:26.654151Z",
     "start_time": "2024-09-09T03:55:20.967859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"\"\"\n",
    "- Write a function named learn that takes a dataframe and an index as parameters.\n",
    "- The index indicates the round number of learning. The function should:\n",
    "- 1. Create a dictionary to store the measurements for this round\n",
    "- 2. Print a message indicating the round number\n",
    "- 3. Store the round number in the dictionary under 'round'\n",
    "- 4. Split the dataframe into code snippets and labels. The code snippets is column 'code' and labels is the column 'language'.\n",
    "- 5. Split the code snippets and labels into training and test sets\n",
    "- 6. Create a TF-IDF vectorizer\n",
    "- 7. Use the 'fit_transform' method on the training data to learn the vocabulary and idf, and return term-document matrix.\n",
    "- 8. Use the 'transform' method on the test data to transform documents to document-term matrix.\n",
    "- 9. Create a Support Vector Machine classifier\n",
    "- 10. Train the classifier using the training data\n",
    "- 11. Save the model to a file and load it back from a file (to make sure it works)\n",
    "- 12. Use the classifier to predict the labels for the test data\n",
    "- 13. Print the classification report which should be a dictionary\n",
    "- 14. Store the training time in the dictionary under 'train'\n",
    "- 15. Store the prediction time in the dictionary under 'predict'\n",
    "- 16. Store the classification report in the dictionary under 'report'\n",
    "- 17. Add 'accuracy' to the dictionary and set it to the accuracy score from the classification report\n",
    "- 18. Append the dictionary to the measurements list for this round\n",
    "\"\"\"\n",
    "response, messages = converse(prompt, messages)\n",
    "print(response)"
   ],
   "id": "c124ced6c0d695a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.metrics import classification_report\n",
      "import pickle\n",
      "import time\n",
      "\n",
      "# Initialize list to store measurements\n",
      "measurements = []\n",
      "\n",
      "def learn(df, index):\n",
      "    # Create dictionary to store measurements for this round\n",
      "    measurement = {\"round\": index}\n",
      "    \n",
      "    print(f\"Round {index}\")\n",
      "    \n",
      "    # Split dataframe into code snippets and labels\n",
      "    X = df['code']\n",
      "    y = df['language']\n",
      "    \n",
      "    # Split data into training and test sets\n",
      "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "    \n",
      "    # Create TF-IDF vectorizer\n",
      "    vectorizer = TfidfVectorizer()\n",
      "    \n",
      "    # Fit and transform training data\n",
      "    start_train = time.time()\n",
      "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
      "    end_train = time.time()\n",
      "    \n",
      "    # Transform test data\n",
      "    start_predict = time.time()\n",
      "    X_test_tfidf = vectorizer.transform(X_test)\n",
      "    end_predict = time.time()\n",
      "    \n",
      "    # Create Support Vector Machine classifier\n",
      "    clf = SVC()\n",
      "    \n",
      "    # Train classifier\n",
      "    clf.fit(X_train_tfidf, y_train)\n",
      "    \n",
      "    # Save and load model\n",
      "    with open(f\"model_round_{index}.pkl\", \"wb\") as file:\n",
      "        pickle.dump(clf, file)\n",
      "    \n",
      "    with open(f\"model_round_{index}.pkl\", \"rb\") as file:\n",
      "        clf_loaded = pickle.load(file)\n",
      "    \n",
      "    # Predict labels for test data\n",
      "    y_pred = clf_loaded.predict(X_test_tfidf)\n",
      "    \n",
      "    # Print classification report\n",
      "    report = classification_report(y_test, y_pred, output_dict=True)\n",
      "    print(report)\n",
      "    \n",
      "    # Store measurements in dictionary\n",
      "    measurement[\"train\"] = end_train - start_train\n",
      "    measurement[\"predict\"] = end_predict - start_predict\n",
      "    measurement[\"report\"] = report\n",
      "    measurement[\"accuracy\"] = report['accuracy']\n",
      "    \n",
      "    # Append dictionary to measurements list\n",
      "    measurements.append(measurement)\n",
      "\n",
      "# Example usage\n",
      "# learn(df, 1)\n",
      "# learn(df, 2)\n",
      "```\n"
     ]
    }
   ],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T03:55:36.685477Z",
     "start_time": "2024-09-09T03:55:34.513855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"\"\"\n",
    "- Load all data files matching 'data/datasets/train-00000-5k*.parquet'. * represents a wildcard.\n",
    "- For each file loaded, merge the latest data file with the merged data to date,\n",
    "- and call the learn function with the dataframe and the index of the file in the list of files.\n",
    "- The idea is for round 1, load in dataset 0. Then for round 2, load in datasets 0,1. Then for round 3, load in datasets 0,1,2. Only perform for 3 rounds.\n",
    "\"\"\"\n",
    "response, messages = converse(prompt, messages)\n",
    "print(response)"
   ],
   "id": "ae1a20d4982378ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import glob\n",
      "import pandas as pd\n",
      "\n",
      "# Load all data files matching the pattern\n",
      "files = glob.glob('data/datasets/train-00000-5k*.parquet')\n",
      "\n",
      "# Initialize merged data\n",
      "merged_data = pd.DataFrame()\n",
      "\n",
      "# Perform for 3 rounds\n",
      "for i in range(3):\n",
      "    # Load the latest data file\n",
      "    file = files[i]\n",
      "    df = pd.read_parquet(file)\n",
      "    \n",
      "    # Merge latest data file with merged data\n",
      "    merged_data = pd.concat([merged_data, df])\n",
      "    \n",
      "    # Call learn function with dataframe and index\n",
      "    learn(merged_data, i)\n",
      "```\n"
     ]
    }
   ],
   "execution_count": 121
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T03:10:22.837201Z",
     "start_time": "2024-09-09T03:10:19.288201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"\"\"\n",
    "I am getting following error for the above piece of code you provided:\n",
    "---------------------------------------------------------------------------\n",
    "KeyError                                  Traceback (most recent call last)\n",
    "File ~/PycharmProjects/AI-Tools/Week2/project2/venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805, in Index.get_loc(self, key)\n",
    "   3804 try:\n",
    "-> 3805     return self._engine.get_loc(casted_key)\n",
    "   3806 except KeyError as err:\n",
    "\n",
    "File index.pyx:167, in pandas._libs.index.IndexEngine.get_loc()\n",
    "\n",
    "File index.pyx:196, in pandas._libs.index.IndexEngine.get_loc()\n",
    "\n",
    "File pandas/_libs/hashtable_class_helper.pxi:7081, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
    "\n",
    "File pandas/_libs/hashtable_class_helper.pxi:7089, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
    "\n",
    "KeyError: 'code_snippets'\n",
    "\n",
    "The above exception was the direct cause of the following exception:\n",
    "\n",
    "KeyError                                  Traceback (most recent call last)\n",
    "Cell In[26], line 19\n",
    "     16 merged_data = pd.concat([merged_data, df], ignore_index=True)\n",
    "     18 # Call learn function with dataframe and index\n",
    "---> 19 learn(merged_data, index)\n",
    "\n",
    "Cell In[25], line 15, in learn(df, index)\n",
    "     12 print(f\"Round {index}\")\n",
    "     14 # Split dataframe into code snippets and labels\n",
    "---> 15 X = df['code_snippets']\n",
    "     16 y = df['labels']\n",
    "     18 # Split code snippets and labels into training and test sets\n",
    "\n",
    "File ~/PycharmProjects/AI-Tools/Week2/project2/venv/lib/python3.12/site-packages/pandas/core/frame.py:4102, in DataFrame.__getitem__(self, key)\n",
    "   4100 if self.columns.nlevels > 1:\n",
    "   4101     return self._getitem_multilevel(key)\n",
    "-> 4102 indexer = self.columns.get_loc(key)\n",
    "   4103 if is_integer(indexer):\n",
    "   4104     indexer = [indexer]\n",
    "\n",
    "File ~/PycharmProjects/AI-Tools/Week2/project2/venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812, in Index.get_loc(self, key)\n",
    "   3807     if isinstance(casted_key, slice) or (\n",
    "   3808         isinstance(casted_key, abc.Iterable)\n",
    "   3809         and any(isinstance(x, slice) for x in casted_key)\n",
    "   3810     ):\n",
    "   3811         raise InvalidIndexError(key)\n",
    "-> 3812     raise KeyError(key) from err\n",
    "   3813 except TypeError:\n",
    "   3814     # If we have a listlike key, _check_indexing_error will raise\n",
    "   3815     #  InvalidIndexError. Otherwise we fall through and re-raise\n",
    "   3816     #  the TypeError.\n",
    "   3817     self._check_indexing_error(key)\n",
    "\n",
    "KeyError: 'code_snippets'\n",
    "\"\"\"\n",
    "\n",
    "response, messages = converse(prompt, messages)\n",
    "print(response)"
   ],
   "id": "7a22d74afc0f49cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error indicates that the column 'code_snippets' does not exist in the DataFrame `df` that is being passed to the `learn` function. \n",
      "\n",
      "To resolve this issue, you need to check the column names in the DataFrame `df` loaded from the parquet file and ensure that it contains the columns 'code_snippets' and 'labels'. If the column names are different, you should adjust the code to use the correct column names.\n",
      "\n",
      "You can print the column names of the DataFrame `df` before calling the `learn` function to verify the column names. Here is an updated version of the code with added print statements to check the column names:\n",
      "\n",
      "```python\n",
      "import glob\n",
      "import pandas as pd\n",
      "\n",
      "# Load all data files matching the pattern\n",
      "data_files = glob.glob('data/datasets/train-00000-5k*.parquet')\n",
      "\n",
      "# Initialize merged data\n",
      "merged_data = pd.DataFrame()\n",
      "\n",
      "for i, file in enumerate(data_files):\n",
      "    # Load data file\n",
      "    df = pd.read_parquet(file)\n",
      "    \n",
      "    # Print column names to verify\n",
      "    print(df.columns)\n",
      "    \n",
      "    # Merge latest data file with merged data\n",
      "    merged_data = pd.concat([merged_data, df], ignore_index=True)\n",
      "    \n",
      "    # Call learn function with dataframe and index\n",
      "    learn(merged_data, i)\n",
      "```\n",
      "\n",
      "By checking the column names, you can ensure that the correct columns are being used in the `learn` function.\n"
     ]
    }
   ],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T04:06:32.045803Z",
     "start_time": "2024-09-09T04:06:27.574615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"\"\"\n",
    "- If I have measurements in Python like a list of dictionaries such as:\n",
    "- `[{'round': 0, 'train': 32.76, 'predict': 2.13, 'accuracy': 0.78},....]`\n",
    "- Multiply accuracy by 100 and round it to 2 decimal places.\n",
    "- let us plot lines on the same graph for train, predict and accuracy using python?  Use matplotlib.\n",
    "- Add a legend.  Add axis labels.  Add a title.\n",
    "- Let us show the plot in this notebook below.\n",
    "\"\"\"\n",
    "response, messages = converse(prompt, messages)\n",
    "print(response)"
   ],
   "id": "72b543fca92ac1fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# List of measurements\n",
      "measurements = [{'round': 0, 'train': 32.76, 'predict': 2.13, 'accuracy': 0.78},\n",
      "                {'round': 1, 'train': 28.45, 'predict': 1.98, 'accuracy': 0.82},\n",
      "                {'round': 2, 'train': 25.67, 'predict': 1.85, 'accuracy': 0.87}]\n",
      "\n",
      "# Multiply accuracy by 100 and round to 2 decimal places\n",
      "for measurement in measurements:\n",
      "    measurement['accuracy'] = round(measurement['accuracy'] * 100, 2)\n",
      "\n",
      "# Extract data for plotting\n",
      "rounds = [m['round'] for m in measurements]\n",
      "train_times = [m['train'] for m in measurements]\n",
      "predict_times = [m['predict'] for m in measurements]\n",
      "accuracies = [m['accuracy'] for m in measurements]\n",
      "\n",
      "# Plot lines on the same graph\n",
      "plt.plot(rounds, train_times, label='Train Time')\n",
      "plt.plot(rounds, predict_times, label='Predict Time')\n",
      "plt.plot(rounds, accuracies, label='Accuracy')\n",
      "\n",
      "# Add legend, axis labels, and title\n",
      "plt.legend()\n",
      "plt.xlabel('Round')\n",
      "plt.ylabel('Time (s) / Accuracy (%)')\n",
      "plt.title('Performance Metrics Over Rounds')\n",
      "\n",
      "# Show the plot\n",
      "plt.show()\n",
      "```\n"
     ]
    }
   ],
   "execution_count": 123
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
